 #1
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow import keras

#2
boston = keras.datasets.boston_housing
(X_train, y_train), (X_test, y_test) = boston.load_data(test_split=0.2, seed=42)
mean = X_train.mean(axis=0)
std = X_train.std(axis=0)
X_train = (X_train - mean) / std
X_test = (X_test - mean) / std

#3
def evaluate_models(train_sizes):
    mae_scores_dnn = []
    mse_scores_dnn = []
    medae_scores_dnn = []
    mae_scores_lr = []
    mse_scores_lr = []
    medae_scores_lr = []

    for size in train_sizes:
        idx = np.random.choice(X_train.shape[0], int(X_train.shape[0] * size), replace=False)
        X_train_subset, y_train_subset = X_train[idx], y_train[idx]

       
        model = keras.Sequential([
            keras.layers.Dense(64, activation='relu', input_shape=(X_train_subset.shape[1],)),
            keras.layers.Dense(1)
        ])
        model.compile(optimizer='adam', loss='mse')
        model.fit(X_train_subset, y_train_subset, epochs=100, verbose=0)
        y_pred_dnn = model.predict(X_test)

        
        lr_model = np.linalg.lstsq(X_train_subset, y_train_subset, rcond=None)[0]
        y_pred_lr = np.dot(X_test, lr_model)

       
        mae_scores_dnn.append(np.mean(np.abs(y_test - y_pred_dnn)))
        mse_scores_dnn.append(np.mean(np.square(y_test - y_pred_dnn)))
        medae_scores_dnn.append(np.median(np.abs(y_test - y_pred_dnn)))
        mae_scores_lr.append(np.mean(np.abs(y_test - y_pred_lr)))
        mse_scores_lr.append(np.mean(np.square(y_test - y_pred_lr)))
        medae_scores_lr.append(np.median(np.abs(y_test - y_pred_lr)))

    return mae_scores_dnn, mse_scores_dnn, medae_scores_dnn, mae_scores_lr, mse_scores_lr, medae_scores_lr

#4
train_sizes = np.linspace(0.1, 1.0, 10)


mae_scores_dnn, mse_scores_dnn, medae_scores_dnn, mae_scores_lr, mse_scores_lr, medae_scores_lr = evaluate_models(train_sizes)

#5
data = [
    (mae_scores_dnn, 'DNN MAE'),
    (mse_scores_dnn, 'DNN MSE'),
    (medae_scores_dnn, 'DNN MedAE'),
    (mae_scores_lr, 'LR MAE'),
    (mse_scores_lr, 'LR MSE'),
    (medae_scores_lr, 'LR MedAE')
]


for scores, label in data:
    plt.figure()
    plt.plot(train_sizes, scores)
    plt.xlabel('Training Size')
    plt.ylabel(label)
    plt.title(f'{label} vs Training Size')
    plt.show()




The provided code performs a comparison between a Deep Neural Network (DNN) and a Linear Regression (LR) model using the Boston Housing dataset. Here's an explanation of each step:

Importing the necessary libraries: NumPy, Matplotlib, TensorFlow, and Keras.
Loading the Boston Housing dataset using the boston_housing module from Keras. It splits the data into training and test sets, standardizes the features using mean and standard deviation, and assigns the standardized data to X_train, y_train, X_test, and y_test.
Defining a function evaluate_models that takes a list of training sizes as input. This function evaluates the performance of DNN and LR models for different training sizes. It calculates Mean Absolute Error (MAE), Mean Squared Error (MSE), and Median Absolute Error (MedAE) for both models.
Generating a list of training sizes (train_sizes) using np.linspace to create evenly spaced values between 0.1 and 1.0.
Calling the evaluate_models function with train_sizes as input. It returns the scores for different metrics (MAE, MSE, MedAE) for both DNN and LR models.
Creating a list of tuples (data) containing the scores and labels for each metric and model.
Plotting the scores for each metric against the training sizes using Matplotlib. It iterates over the data list and creates separate plots for each metric. Each plot shows how the score varies with the training size.
Overall, the code performs the following tasks: loading and preprocessing the Boston Housing dataset, defining and training DNN and LR models, evaluating the models' performance for different training sizes, and visualizing the scores using plots. This allows for a comparison of the models' performance as the training size varies.






